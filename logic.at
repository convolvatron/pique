  section{Pique}{

  This is a little project to build a dataflow graph from a rule set. Its exploring the thesis (present in Verse)
  that logic programming isn't really that different than functional programming, its just been obscured by
  the syntactic conventions around datalog implementations. Call pique an ergnomic experiment.

  So to explore this a little, we make a very basic top down streaming datalog implementation, with rules specified
  using native python syntax.

  The functional subset of python involves
  list{
    item{no reassignment of variables}
    item{no side effects}
  }
  

  section{logic programing}{
  
    These are characteristic about logic programming that make it a little different from functional programming
      list{
        item{tables of data are also just functions}
        item{statments are order independent}
        item{functions may not return a value at all, or may return many values}
        item{argumnets can serve as inputs or outputs}
      }
      
      section{the implicit union of multiple definitions}{
        the standard datalog construction allows us to define a
        function multiple times, and the result at any call site is
        the union of the results of all of them. this seems odd, but
        its really quite useful if you consider that many logic
        programs are 'run backwards'.

        we follow this model in Pique, but it would be interesting to think
        about altneratives that hew more closely to peoples expectations. It
        also brings up the unfortunate requirement to enforce that all implementations
        of a given rule have matching signatures. It also makes it strange to think
        about redefining a rule dynamically.

        ok, we're going to consider an alternative, which allows us to assert()
        a value, with semantics equivalent to having it in a rule head (since
        everything is bidirectional anyways). this also allows us to have multiple
        'heads' in the same body, which is an additional convenience
      }
      
   }
    
  section{Evaluation}
     on a relation invocation -
        map the callers domain into the callee for bound arguments
        map the callee into the caller on return
        
     
  section{Python Embedding}{
     we add and annotation to python that is intepreted by
     load.py. code {@rule} puts a functoin definition in the rule
     namespace
     
     list{
        item{rule}{a horn clause expressed as a python function def}
        item{fact}{a statement about the world, expressed as a python callable}        
     }
     rules and facts should be in disjoint relations
       section{translation}{
         normaly we would want to resolve signatures and relations at compile time, but
         its simpler for this demo to do both at eval time.
       }
  }


  section{Relations}{
    Relations are a set of tuples, including the arguments and the
    results, that are members. You can see how this isn't so different
    from the functional model of a mapping from inputs to outputs. What it lets us do
    thats different is:
       list{
         item{easily integrate data sources which are exactly such sets (or bags)}
         item{run functions backwards}     
       }
  }

  section{Rule Semantics}{

    We have a choice of use set semantics, where each unique tuple is
    defined only once, or bag, which allows an arbitrary number of
    copies. Databases generally use bag semantics, but we're going to use
    set here since its a little easier to reason about. You can express
    bag semantics in set by replacing tuples with a code{(count, tuple)} pair.

    section{result sets}{
       In logic programming, variables and rules dont denote a a
       single value, but all possible values that satistfy the
       constraints. This is the major operational difference, and
       removes alot of boilerplate and fussy loop
       construction. However, its make it a little harder to reason
       about and construct complicated reductions with inter-value
       dependencies.}}
   

  section{Signatures}{
    In introductory algebra we learned to solve a system of
    equations. For each equation, we can rerwite it to solve some for
    some variable given the others, and thats the same kind of
    structure we're going to apply here.
    
    In order to map relations to an underlying procedural implementation, we break out each of
    the sets of bound arguments into a separate function. That is a relation over (a, b, c)
    may have implementations for:
    

    comment{tables?}
    
     list{
        item{code{() -> (a, b c)}}
        item{code{(a) -> (b, c)}}
        item{code{(a, b) -> c}}
        item{code{(a, b, c)->()}}
        item{code{(b, c) -> a}}
        item{...}
     }

    For some relations, such as rules and external databases, we can generate all these permutations
    automatically without much struggle. For functionally backed relations, we may only choose to implement

     section{Signature Implementations in Python}{
     }
   }

  section{Excercises for the reader}{
  
    section{incremental evalutaion}{
      one of the really cool side effects of 
    }

    
    section{cyclic evaluation and negation}{
    
    }
   
    section{reordering compiler}{
      As we mentioned, the semantics of rules allow us to evaluate each of the statements in any order.
      Some orderings are quite a bit more efficient than others. For example a join between two relations
      is generally much faster if we iterate over the smallest one first. Filters that potentially discard
      result tuples should be applied as early as possible. This is exactly analogous to the standard join
      ordering problem.

      Further, there may be signatures that dont make sense operationally. For example code{a = sum(b)} as a reduction
      doesn't make sense if code{b} isn't bound - we end up enumerating all possible sets, or all possible sets
      whose sum is code{a}.

      So, aside from evaluation efficiency, the reorderer can make this situation more ergonomic by taking
      any program, and only emmiting an actual evulation order which has matching signatures (is comptuable
      without generating any intermediate infinite results).

      Of course, its still possible to write rules which cant be effectively evaluated, although
      some of them may have a viable strategy that could be found manually, or possibly with 
      a theorem prover.
    }


    section{named arguments}{
      for sake of brevity, we only considered the subset of the python calling model that uses
      unnamed positional arguments. support for variadic rules (*args) and keyword arguments with
      optinal defaults. the latter is pretty trivial if you just dump the calling argumnets into
      the callees namespace. this can be used as the general calling convention if you're willing
      to map the position arguments into keywords (i.e. '0', '1', '2', etc).

      named arguments allow us a more recognizable and useful implementation of standard relational
      tables. we hedged this a tiny bit by giving the positional arguments names.
     }

    section{arrays as frames}{
      for brevity and introspection, we pass around python dictionaries to represent the result
      tuples under consideration. its quite a bit more efficient to do a compiler pass and
      map these into slots in a contiguous array. if we have static typing information, then
      some or all of these values can be inlined or mapped to registers, creating a per-function
      bespoke calling convention.

      section{batching}{
          we can take this a step further, amortize our control transfer overhead, and improve
          data locality by adding an additional level of grouping and processing the inputs
          to signature handlers as batches. we can also consider column-wise batch storage
          and that would lead us to the edge of being able to exploit vector execution.
       }
     }

    section{distributed streams}{
       you can notice that the evaluation of the output dataflow graph is always feed-forward.
       This makes is fairly straightforward to extend the evaluation of the dataflow graph
       between machines on a network (or pipeline threads in a multithreaded system).

       in such a system, it is almost mandatory to provide backpressure. in our simple
       stack oriented evaluation there is only one thread, but if there are multiple processes
       working on parts of the graphs at different rates, over time intermediate results
       can pile up at the head of relatively slower processors. this unbounded memory
       consumption can become a real operational problem. one common method of addressing this
       is to implement a credit scheme, where downstream nodes issue credits backwards along
       the dataflow graph in order the limit the number of outstanding results. the optimal
       number of credits is basically the bandwidth-delay product.
    }

    section{lower to a compiled language or directly to assembly}{
       this demo constructs a dataflow graph out of python closures, and uses calling conventions
       and iterators to run it all. while convenient, that isnt particularily efficient. One altenative
       to returning an iterator from a signature 
       
    }

    section{generalized transforms for set construction and deconstruction}{
      as we mentioned in the ref{result sets} section, one of the things
      that implementors of logic programming struggle with is the
      expression of reductions (aggregates in databaseland). Quite of
      a bit of this is cultural - SQL has a very strange construction around
      code{GROUP BY} that really doesn't generalize and is a spooky action at a distance.
    }



    section{type hint integrations}{
      logic programming has a really nice integration with typing - assuming you are willing to
      adopt a set-based interpretation for programming types. That is type annotations can
      be interpreted directly as additional constraint clauses in the body. While this is
      nice semantically, it mixes in information that the compiler has a very specific interest in
      with the rest of the stuff.

      this also opens up the pleasant possiblility of having free reign to adjust your type system
      (looks like liquid types has a similar if more formally specified goal)
    }


     section{single static assignment}{
       in our toy system, we unify all instances of a variable occurance. but thats not really
       what python variables are - they are .. variable. we dont expect that if we say
       code{
         a = b + 5
         a = a + 1
       }
       that there will be no results, because there is no solution. we expect a to by code{b+6}

       we can get back this behaviour by using a construction called static single assignment
       which renames the second assignment of a to be code{a'} and replaces uses of code{a}
       with code{a'} in successive statements. (its not that straightforward in the presence
       of control flow, so look it up)
     }
  }

   section{notebeook integration / development}{}
   section{better notation for base facts}{}
   section{integration with external fact sources}{
   list{
     item{postgres}
     item{json}     
   }
   section{allow rules to be expressed as data objects}{}
}